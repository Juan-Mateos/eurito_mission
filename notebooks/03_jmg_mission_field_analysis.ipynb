{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mission field analysis\n",
    "\n",
    "We start the analisys of the enriched GTR dataset (see `01` and `02` notebooks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = pd.read_csv('../data/processed/22_1_2019_projects_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word embeddings query / Clio query?\n",
    "\n",
    "To keep things simple, we will train a w2v model, identify synonyms for a set of seed terms and query the data for those"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/utils/__init__.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working\n",
      "  from collections import Sequence\n"
     ]
    }
   ],
   "source": [
    "# %load lda_pipeline.py\n",
    "from gensim import corpora, models\n",
    "from string import punctuation\n",
    "from string import digits\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "#Characters to drop\n",
    "drop_characters = re.sub('-','',punctuation)+digits\n",
    "\n",
    "#Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('English')\n",
    "\n",
    "#Stem functions\n",
    "from nltk.stem import *\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def flatten_list(a_list):\n",
    "    return([x for el in a_list for x in el])\n",
    "\n",
    "\n",
    "def clean_tokenise(string,drop_characters=drop_characters,stopwords=stop):\n",
    "    '''\n",
    "    Takes a string and cleans (makes lowercase and removes stopwords)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "\n",
    "    #Lowercase\n",
    "    str_low = string.lower()\n",
    "    \n",
    "    \n",
    "    #Remove symbols and numbers\n",
    "    str_letters = re.sub('[{drop}]'.format(drop=drop_characters),'',str_low)\n",
    "    \n",
    "    \n",
    "    #Remove stopwords\n",
    "    clean = [x for x in str_letters.split(' ') if (x not in stop) & (x!='')]\n",
    "    \n",
    "    return(clean)\n",
    "\n",
    "\n",
    "class CleanTokenize():\n",
    "    '''\n",
    "    This class takes a list of strings and returns a tokenised, clean list of token lists ready\n",
    "    to be processed with the LdaPipeline\n",
    "    \n",
    "    It has a clean method to remove symbols and stopwords\n",
    "    \n",
    "    It has a bigram method to detect collocated words\n",
    "    \n",
    "    It has a stem method to stem words\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes a corpus (list where each element is a string)\n",
    "        '''\n",
    "        \n",
    "        #Store\n",
    "        self.corpus = corpus\n",
    "        \n",
    "    def clean(self,drop=drop_characters,stopwords=stop):\n",
    "        '''\n",
    "        Removes strings and stopwords, \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        cleaned = [clean_tokenise(doc,drop_characters=drop,stopwords=stop) for doc in self.corpus]\n",
    "        \n",
    "        self.tokenised = cleaned\n",
    "        return(self)\n",
    "    \n",
    "    def stem(self):\n",
    "        '''\n",
    "        Optional: stems words\n",
    "        \n",
    "        '''\n",
    "        #Stems each word in each tokenised sentence\n",
    "        stemmed = [[stemmer.stem(word) for word in sentence] for sentence in self.tokenised]\n",
    "    \n",
    "        self.tokenised = stemmed\n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def bigram(self,threshold=10):\n",
    "        '''\n",
    "        Optional Create bigrams.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Colocation detector trained on the data\n",
    "        phrases = models.Phrases(self.tokenised,threshold=threshold)\n",
    "        \n",
    "        bigram = models.phrases.Phraser(phrases)\n",
    "        \n",
    "        self.tokenised = bigram[self.tokenised]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "class LdaPipeline():\n",
    "    '''\n",
    "    This class processes lists of keywords.\n",
    "    How does it work?\n",
    "    -It is initialised with a list where every element is a collection of keywords\n",
    "    -It has a method to filter keywords removing those that appear less than a set number of times\n",
    "    \n",
    "    -It has a method to process the filtered df into an object that gensim can work with\n",
    "    -It has a method to train the LDA model with the right parameters\n",
    "    -It has a method to predict the topics in a corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,corpus):\n",
    "        '''\n",
    "        Takes the list of terms\n",
    "        '''\n",
    "        \n",
    "        #Store the corpus\n",
    "        self.tokenised = corpus\n",
    "        \n",
    "    def filter(self,minimum=5):\n",
    "        '''\n",
    "        Removes keywords that appear less than 5 times.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Count tokens\n",
    "        token_counts = pd.Series([x for el in tokenised for x in el]).value_counts()\n",
    "        \n",
    "        #Tokens to keep\n",
    "        keep = token_counts.index[token_counts>minimum]\n",
    "        \n",
    "        #Filter\n",
    "        tokenised_filtered = [[x for x in el if x in keep] for el in tokenised]\n",
    "        \n",
    "        #Store\n",
    "        self.tokenised = tokenised_filtered\n",
    "        self.empty_groups = np.sum([len(x)==0 for x in tokenised_filtered])\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def clean(self):\n",
    "        '''\n",
    "        Remove symbols and numbers\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "    def process(self):\n",
    "        '''\n",
    "        This creates the bag of words we use in the gensim analysis\n",
    "        \n",
    "        '''\n",
    "        #Load the list of keywords\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #Create the dictionary\n",
    "        dictionary = corpora.Dictionary(tokenised)\n",
    "        \n",
    "        #Create the Bag of words. This converts keywords into ids\n",
    "        corpus = [dictionary.doc2bow(x) for x in tokenised]\n",
    "        \n",
    "        self.corpus = corpus\n",
    "        self.dictionary = dictionary\n",
    "        return(self)\n",
    "        \n",
    "    def tfidf(self):\n",
    "        '''\n",
    "        This is optional: We extract the term-frequency inverse document frequency of the words in\n",
    "        the corpus. The idea is to identify those keywords that are more salient in a document by normalising over\n",
    "        their frequency in the whole corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Fit a TFIDF model on the data\n",
    "        tfidf = models.TfidfModel(corpus)\n",
    "        \n",
    "        #Transform the corpus and save it\n",
    "        self.corpus = tfidf[corpus]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "    def fit_lda(self,num_topics=20,passes=5,iterations=75,random_state=1803):\n",
    "        '''\n",
    "        \n",
    "        This fits the LDA model taking a set of keyword arguments.\n",
    "        #Number of passes, iterations and random state for reproducibility. We will have to consider\n",
    "        reproducibility eventually.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Load the corpus\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Train the LDA model with the parameters we supplied\n",
    "        lda = models.LdaModel(corpus,id2word=self.dictionary,\n",
    "                              num_topics=num_topics,passes=passes,iterations=iterations,random_state=random_state)\n",
    "        \n",
    "        #Save the outputs\n",
    "        self.lda_model = lda\n",
    "        self.lda_topics = lda.show_topics(num_topics=num_topics)\n",
    "        \n",
    "\n",
    "        return(self)\n",
    "    \n",
    "    def predict_topics(self):\n",
    "        '''\n",
    "        This predicts the topic mix for every observation in the corpus\n",
    "        \n",
    "        '''\n",
    "        #Load the attributes we will be working with\n",
    "        lda = self.lda_model\n",
    "        corpus = self.corpus\n",
    "        \n",
    "        #Now we create a df\n",
    "        predicted = lda[corpus]\n",
    "        \n",
    "        #Convert this into a dataframe\n",
    "        predicted_df = pd.concat([pd.DataFrame({x[0]:x[1] for x in topics},\n",
    "                                              index=[num]) for num,topics in enumerate(predicted)]).fillna(0)\n",
    "        \n",
    "        self.predicted_df = predicted_df\n",
    "        \n",
    "        return(self)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create sentence corpus\n",
    "sentence_corpus = flatten_list([x.split('. ') for x in projects['abstract']])\n",
    "\n",
    "\n",
    "#Tokenize etc using the classes above\n",
    "sentence_tokenised = CleanTokenize(sentence_corpus).clean().bigram()\n",
    "\n",
    "#Also tokenise by documents so we can query them later\n",
    "corpus_tokenised = CleanTokenize(projects['abstract']).clean().bigram()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training W2V\n",
    "w2v = Word2Vec(sentence_tokenised.tokenised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../models/{today_str}_word_embeddings.p','wb') as outfile:\n",
    "    pickle.dump(w2v,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_chaser(seed_list,model,similarity,occurrences=1):\n",
    "    '''\n",
    "    Takes a seed term and expands it with synonyms (above a certain similarity threshold)\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #All synonyms of the terms in the seed_list above a certain threshold\n",
    "    set_ws = flatten_list([[term[0] for term in model.most_similar(seed) if term[1]>similarity] for seed in seed_list])\n",
    "    \n",
    "    #return(set_ws)\n",
    "    \n",
    "    #This is the list of unique occurrences (what we want to return at the end)\n",
    "    set_ws_list = list(set(set_ws))\n",
    "    \n",
    "    #For each term, if it appears multiple times, we expand\n",
    "    for w in set_ws:\n",
    "        if set_ws.count(w)>occurrences:\n",
    "            \n",
    "            #As before\n",
    "            extra_words = [term[0] for term in model.wv.most_similar(w) if term[1]>similarity]\n",
    "            \n",
    "            set_ws_list + extra_words\n",
    "            \n",
    "    #return(list(set(set_ws_list)))\n",
    "    #return(set_ws_list)\n",
    "    return(set_ws)\n",
    "\n",
    "    \n",
    "def querier(corpus,keywords,intersect=True):\n",
    "    '''\n",
    "    Loops over a tokenised corpus and returns the number of hits (number of times that any of the terms appears in the document)\n",
    "    \n",
    "    '''\n",
    "    #Intersection of tokens\n",
    "    if intersect==True:\n",
    "    \n",
    "        out = [len(set(keywords) & set(document)) for document in corpus]\n",
    "    \n",
    "    else:\n",
    "    #Otherwise it counts the total of tokens present in an abstract\n",
    "        \n",
    "        out = [np.sum([x.count(k) for k in keywords]) for x in corpus]\n",
    "    \n",
    "    \n",
    "    \n",
    "    return(out)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI and Chronic diseases (crude keyword search-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_check(corpus,num,length):\n",
    "    '''\n",
    "    Prints num random examples form corpus\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    selected = np.random.randint(0,len(corpus),num)\n",
    "    \n",
    "    texts  = [text for num,text in enumerate(corpus) if num in selected]\n",
    "    \n",
    "    for t in texts:\n",
    "        print(t[:length])\n",
    "        print('====')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class missionClassifier():\n",
    "    '''\n",
    "    \n",
    "    input: a list of projects with descriptions, dates, funding and outputs.\n",
    "    \n",
    "    -Expands keywords in a model (this could be clio or something else)\n",
    "    -Uses those queries to identify relevant projects\n",
    "    -Generates some descriptive statistics about the projects\n",
    "    -Generates some graphs about the projects.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,projects,corpus_tokenised,model):\n",
    "        '''\n",
    "        \n",
    "        Initialises the class with the projects and the w2v model we will use.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.projects = projects\n",
    "\n",
    "        #This is the project df but with text description tokenised. We query it with the keywords \n",
    "        \n",
    "        self.tokenised = corpus_tokenised\n",
    "        self.w2v = model\n",
    "        \n",
    "        \n",
    "    def keyword_expansion(self,mission_dict,thres):\n",
    "        '''\n",
    "        \n",
    "        Expands a seed list of keywords. We input those as a dict where key corresponds to the type of input (solution or\n",
    "        challenge, say) and the values are a list with the name of the entity (eg 'AI') and the seedlist to expand\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Store the names eg 'challenge_chronic'\n",
    "        self.names = [v[0] for k,v in mission_dict.items()]\n",
    "        \n",
    "        #Store the keywords\n",
    "        self.keywords = [v[1] for k,v in mission_dict.items()]\n",
    "        \n",
    "        #Expand the keywords\n",
    "        self.expanded_keywords = [list(set(kw + similarity_chaser(seed_list=kw,model=self.w2v,similarity=thres))) for\n",
    "                                 kw in self.keywords]\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "    \n",
    "    def query_data(self,intersect=True,verbose=False):\n",
    "        '''\n",
    "        Queries the data with the keywords\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        projects_labelled = self.projects.copy()\n",
    "        tokenised = self.tokenised\n",
    "        \n",
    "        #We look for projects with keywords. Loop over name and extract the right index from the expanded keyword set.\n",
    "        #This could also work as a dict.\n",
    "        \n",
    "        for num,name in enumerate(self.names):\n",
    "            \n",
    "            if verbose==True:\n",
    "                print(name)\n",
    "            \n",
    "            #Note that if intersection=True the values of the variable represent the number of query hits that a \n",
    "            #project has received, a proxy of its relevance.\n",
    "            \n",
    "            projects_labelled[name] = querier(tokenised,self.expanded_keywords[num],intersect=intersect)\n",
    "            \n",
    "        self.projects_labelled = projects_labelled\n",
    "        \n",
    "        return(self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class missionProfiler(missionClassifier):\n",
    "    '''\n",
    "    Takes as input an instance of the class missionClassifier\n",
    "\n",
    "    -Generates some descriptive statistics about the projects\n",
    "    -Generates some graphs about the projects.\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,missionClassifier):\n",
    "        '''\n",
    "        \n",
    "        Initialise\n",
    "        '''\n",
    "        \n",
    "        self.projects_labelled = missionClassifier.projects_labelled\n",
    "        self.names = missionClassifier.names\n",
    "        \n",
    "        self.column_names = [self.names[0]+'_and_'+self.names[1],self.names[0],self.names[1]]\n",
    "        \n",
    "    def mission_examples(self,n,length,text_var='abstract',thr=0):\n",
    "        '''\n",
    "        Prints n examples of missions using the text variable that was specified\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        names = self.names\n",
    "        \n",
    "        for name in names:\n",
    "            print(name)\n",
    "            print('\\n')\n",
    "            \n",
    "            random_check(corpus=list(self.projects_labelled.loc[self.projects_labelled[name]>thr,text_var]),\n",
    "                         num=n,\n",
    "                         length=200)\n",
    "            \n",
    "            print('\\n')\n",
    "        \n",
    "        print(names[0]+' and ' + names[1])\n",
    "        print('\\n')\n",
    "        random_check(corpus=list(self.projects_labelled.loc[\n",
    "            (self.projects_labelled[names[0]]>thr) & (self.projects_labelled[names[1]]>thr),text_var]),\n",
    "                         num=n,\n",
    "                         length=200)\n",
    "            \n",
    "        \n",
    "        \n",
    "    def mission_field_basics(self,field_filter=[0,0],verbose=True):\n",
    "        '''\n",
    "        Generates estimates of activity in a field. field_filter refers to whether\n",
    "        '''\n",
    "        \n",
    "        projects = self.projects_labelled\n",
    "        names = self.names\n",
    "        \n",
    "        #Do the filter. Note that we assume that the user here will have done some EDA of the results to choose the right\n",
    "        #filter.\n",
    "        \n",
    "        #Binarise. Note that the default assumes that the values here are a \n",
    "        for num,n in enumerate(names):\n",
    "            projects[n] = projects[n].apply(lambda x: 1 if x> field_filter[num] else 0)\n",
    "            \n",
    "        \n",
    "        \n",
    "        #What's the size of the potential and effective mission field?\n",
    "        #TODO: generalise to more than two fields.\n",
    "        \n",
    "        #Potential mission field\n",
    "        union = projects.loc[(projects[names[0]]==True)|(projects[names[1]]==True),:]\n",
    "\n",
    "        #Active mission field\n",
    "        intersection = projects.loc[(projects[names[0]]==True)& (projects[names[1]]==True),:]\n",
    "\n",
    "        #field 0 totals\n",
    "        field_0 = projects.loc[(projects[names[0]]==True),:]\n",
    "\n",
    "        #Fields 1 totals\n",
    "        field_1 = projects.loc[(projects[names[1]]==True),:]\n",
    "        \n",
    "        #We are not very interested in the union of fields\n",
    "        self.sets = [intersection,field_0,field_1]\n",
    "\n",
    "        summary = {names[0]+'_or_'+names[1]:len(union),\n",
    "                   names[0]+'_and_'+names[1]:len(intersection),\n",
    "                   names[0]+'_total':len(field_0),\n",
    "                   names[1]+'_total':len(field_1)}\n",
    "        \n",
    "        if verbose==True:\n",
    "            print(summary)\n",
    "        \n",
    "        #Store the mission summary\n",
    "        self.mission_summary = summary\n",
    "        \n",
    "               \n",
    "        #Mission basics (over / underrepresentation of )\n",
    "        mission_basics = pd.Series([100*len(intersection)/len(field_0),100*len(field_1)/len(projects),\n",
    "                                    100*len(intersection)/len(field_1),100*len(field_0)/len(projects)],\n",
    "               index=[names[0]+'_and_'+names[1]+'as_share_of_'+names[0],\n",
    "                      names[1]+'_as_share_of all',\n",
    "                      names[0]+'_and_'+names[1]+'_as_share_of'+names[1],\n",
    "                      names[0]+'_as_share_of all'])\n",
    "        \n",
    "        self.mission_basics = mission_basics\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "    def mission_trends(self,years=[2006,2019],funding=False,year_var='year'):\n",
    "        '''\n",
    "        Calculates mission trends over time.\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        #Extract projects\n",
    "        projects =self.projects_labelled\n",
    "        \n",
    "        names = self.names\n",
    "        \n",
    "        #Results dict\n",
    "        \n",
    "        self.trends = {}\n",
    "        \n",
    "        \n",
    "        #####\n",
    "        #Year trends\n",
    "        #####\n",
    "        \n",
    "        #Year counts for each variable of interest\n",
    "        trends = pd.concat([df[year_var].value_counts(normalize=1) for df in self.sets],axis=1)\n",
    "        \n",
    "        trends.columns =self.column_names\n",
    "        \n",
    "        project_trends = trends.loc[(trends.index>years[0]) & (trends.index<years[1])]\n",
    "        \n",
    "        self.trends['project_trends'] = project_trends\n",
    "        \n",
    "        \n",
    "        #And also look at funding (if available)\n",
    "        if funding!=False:\n",
    "            \n",
    "            #This calculates totals of funding and normalises over the interval to see if recent periods have more activity\n",
    "            \n",
    "            funding_trends = pd.concat([df.groupby(year_var)[funding].sum() for df in self.sets],axis=1)\n",
    "            \n",
    "            funding_trends.columns = self.column_names\n",
    "            \n",
    "            funding_norm = funding_trends.apply(lambda x: x/x.sum(),axis=0)\n",
    "            \n",
    "            funding_norm = funding_norm.loc[(funding_norm.index>years[0]) & (funding_norm.index<years[1])]\n",
    "\n",
    "            self.trends['funding_trends'] = funding_norm\n",
    "    \n",
    "        \n",
    "        ####\n",
    "        #Yearly shares\n",
    "        ####\n",
    "        \n",
    "        #Normalises for each period. NB there is a loooot of repetition below. Need to refactor.\n",
    "        \n",
    "        trends_as_share = pd.concat([df.year.value_counts() for df in self.sets],axis=1)\n",
    "\n",
    "        trends_as_share.columns = self.column_names\n",
    "\n",
    "        #This gets the number of projects in the mission as a share of the total\n",
    "        trends_as_share[\n",
    "            f'mission_field_share_{names[0]}'],trends_as_share[f'mission_field_share_{names[1]}'] = [\n",
    "            trends_as_share.iloc[:,0]/trends_as_share.iloc[:,num] for num in [1,2]]\n",
    "        \n",
    "        trends_as_share = trends_as_share.loc[(trends_as_share.index>years[0]) & (trends_as_share.index<years[1])]\n",
    "        \n",
    "        self.trends['project_shares'] = trends_as_share\n",
    "        \n",
    "        #And also funding (if available)\n",
    "        \n",
    "        if funding != False:\n",
    "            \n",
    "            #We use the funding trend df from above\n",
    "            funding_as_share = funding_trends.copy()\n",
    "            \n",
    "            funding_as_share[\n",
    "                f'mission_field_share_{names[0]}'],funding_as_share[f'mission_field_share_{names[1]}'] = [\n",
    "                funding_as_share.iloc[:,0]/funding_as_share.iloc[:,num] for num in [1,2]]\n",
    "            \n",
    "            funding_as_share = funding_as_share.loc[(funding_as_share.index>years[0]) & (funding_as_share.index<years[1])]\n",
    "            \n",
    "            self.trends['funding_shares'] = funding_as_share\n",
    "            \n",
    "        return(self)\n",
    "            \n",
    "            \n",
    "    def funder_trends(self,years=[2006,2019],funder_var='funder',year_var='year',funding=False):\n",
    "        '''\n",
    "        Funding plots\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        self.funders = {}\n",
    "        \n",
    "        names = self.names\n",
    "        \n",
    "        #####\n",
    "        #Funding totals\n",
    "        ####\n",
    "        \n",
    "        #As before... this gets funder shares of activity\n",
    "        funders = pd.concat([df[funder_var].value_counts(normalize=1) for df in self.sets],axis=1)\n",
    "\n",
    "        funders.columns = self.column_names\n",
    "        \n",
    "        self.funders['funder_projects'] = funders\n",
    "        \n",
    "        #If we want to look at funding\n",
    "        \n",
    "        if funding != False:\n",
    "            funding_shares = pd.concat([df.groupby(funder_var)[funding].sum() for df in self.sets],axis=1)\n",
    "            \n",
    "            funding_shares = funding_shares.apply(lambda x: x/x.sum(),axis=0)\n",
    "            \n",
    "            funding_shares.columns = self.column_names\n",
    "        \n",
    "            self.funders['funder_funding'] = funding_shares\n",
    "\n",
    "        ######\n",
    "        #Funding trends\n",
    "        ######\n",
    "        \n",
    "        mission_field = self.sets[0]\n",
    "        \n",
    "        self.funders['funder_project_trends'] = pd.crosstab(mission_field[year_var],mission_field[funder_var])\n",
    "        \n",
    "        if funding != False:\n",
    "            \n",
    "            funder_funding_trends = mission_field.groupby([year_var,funder_var])[funding].sum()\n",
    "            \n",
    "            \n",
    "            \n",
    "            self.funders['funder_funding_trends'] = pd.pivot_table(\n",
    "                funder_funding_trends.reset_index(drop=False),index=year_var,columns=funder_var,values=funding).fillna(0)\n",
    "            \n",
    "        return(self)\n",
    "            \n",
    "            \n",
    "    def discipline_mix(self,disc_vars = discs,thres=0.1):\n",
    "        '''\n",
    "        Estimates rough measures of interdisciplinarity for projects\n",
    "        '''\n",
    "        \n",
    "        #Calculates the entropy for each project and stores it in the sets\n",
    "        self.sets[0]['entropy'],self.sets[1]['entropy'],self.sets[2]['entropy'], = [\n",
    "            df[disc_vars].apply(entropy,axis=1) for df in self.sets]\n",
    "        \n",
    "        \n",
    "        disc_distr = pd.concat([df[disc_vars].applymap(lambda x: x>thres).sum(axis=1).value_counts(normalize=True) \n",
    "                                  for df in self.sets],axis=1).fillna(0)\n",
    "\n",
    "\n",
    "        disc_distr.columns = self.column_names\n",
    "        \n",
    "        \n",
    "        self.discipline_mix = disc_distr\n",
    "        \n",
    "        return(self)\n",
    "        \n",
    "        \n",
    "\n",
    "    def impact_mix(self,years=[2006,2019],year_var='year',impact_vars=imps):\n",
    "        \n",
    "        '''\n",
    "        Compares impact stats in different fields.\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        imps_share = pd.concat([df.loc[\n",
    "            (df[year_var]>years[0]) & (df[year_var]<years[1]),impact_vars].mean() for df in self.sets],axis=1)\n",
    "        \n",
    "        imps_share.columns = self.column_names\n",
    "\n",
    "        imps_norm = imps_share.T/imps_share.T.mean()\n",
    "        \n",
    "        self.impacts = [imps_share,imps_norm]\n",
    "        \n",
    "        return(self)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These are prototype words. \n",
    "ai_seed = ['machine_learning','artificial_intelligence','deep_learning','ai','machine_vision','text_mining','data_mining']\n",
    "chronic_seed = ['chronic_disease','chronic_condition','addiction','alzheimers','atrial_fibrillation','autoimmune_disease',\n",
    "               'lupus','bipolar_disorder','blindness','cerebral_palsy','chronic_hepatitis','depression','chronic_pain',\n",
    "               'deafness','blindness','endometriosis','epilepsy','hiv','aids','huntingtons','hypertension','lyme',\n",
    "               'sclerosis','parkinsons','sickle_cell']\n",
    "\n",
    "#Discipline terms\n",
    "discs = ['biological_sciences', 'physics', 'engineering_technology',\n",
    "       'medical_sciences', 'social_sciences', 'mathematics_computing',\n",
    "       'environmental_sciences', 'arts_humanities']\n",
    "\n",
    "#Impact terms\n",
    "imps = ['prods', 'ip', 'tech', 'spin', 'pubs']\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classify projects into categories using the expanded keyword search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = missionClassifier(projects,corpus_tokenised.tokenised,w2v)\n",
    "\n",
    "mission_dict = {'solution':['ai',ai_seed],'challenge':['chronic_condition',chronic_seed]}\n",
    "\n",
    "mp.keyword_expansion(mission_dict,thres=0.8).query_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run the mission profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mprof = missionProfiler(mp)\n",
    "\n",
    "mprof.mission_field_basics().mission_trends(funding='amount').funder_trends(funding='amount').discipline_mix().impact_mix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mission field analysis\n",
    "\n",
    "### Descriptive statistics\n",
    "\n",
    "#### Mission field basics\n",
    "\n",
    "A mission field represents the combination of keywords (subjects/verbs/objects) that comprise a mission. \n",
    "\n",
    "For example, the potential mission field $MF_p$ for `using AI (AI) to prevent, diagnose and treat chronic diseases (C)` is formed by $AI \\cup C$. \n",
    "\n",
    "This captures the total of technological activity $AI$ that could be devoted to adressing the challenge $C$.\n",
    "\n",
    "The active mission field $MF_a$ is $AI \\cap C$, capturing the actual number of projects that combine the activities.\n",
    "\n",
    "Some hypotheses:\n",
    "\n",
    "* $\\frac{MF_a}{MF_p} < \\frac{A}{Total}$\n",
    "* $\\frac{MF_a}{MF_p} < \\frac{C}{Total}$\n",
    "\n",
    "This means that the challenge is underrepresented among the applications of the technology, and that the technology is underrepresented among the attempted solutions to the challenge.\n",
    "\n",
    "Note that the above is sort of assuming that the solution field and the challenge field are far apart in the knowledge space. We could test this if we mapped that, but it would require us to classify all projects into their topics. We can sort of proxy this for now by analyzing the organizational network in the three domains. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The solution seems underrepresented in the challenge compared to the total of projects\n",
    "\n",
    "The challenge seems underrepresented in the solution compared to the total of projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mprof.mission_basics.plot.bar(color=['orange','blue','orange','blue'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trends\n",
    "\n",
    "We assume growth in the effective mission field but potentially slower than either of its components (there is some sort of barrier preventing the application of the solution to the challenge).\n",
    "\n",
    "$\\frac{\\delta MF_a}{\\delta t} < \\frac{\\delta A}{\\delta t}$\n",
    "\n",
    "$\\frac{\\delta MF_a}{\\delta t} < \\frac{\\delta C}{\\delta t}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=2,figsize=(12,8),ncols=2,sharex='row')\n",
    "\n",
    "mprof.trends['project_trends'].rolling(window=4).mean().plot(ax=ax[0][0])\n",
    "mprof.trends['funding_trends'].rolling(window=4).mean().plot(ax=ax[1][0])\n",
    "\n",
    "mprof.trends['project_shares'].iloc[:,-2:].rolling(window=4).mean().plot(ax=ax[0][1])\n",
    "mprof.trends['funding_shares'].iloc[:,-2:].rolling(window=4).mean().plot(ax=ax[1][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Funding\n",
    "\n",
    "The funder for the mission fied might be different from the funder for the constituent parts. I would expect the mission funder to be related (closer) to the challenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(4,8),nrows=2)\n",
    "\n",
    "mprof.funders['funder_projects'].T.plot.bar(stacked=True,ax=ax[0],width=0.8)\n",
    "mprof.funders['funder_funding'].T.plot.bar(stacked=True,ax=ax[1],width=0.8)\n",
    "\n",
    "ax[0].legend(bbox_to_anchor=(1,1))\n",
    "ax[1].legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discipline mix\n",
    "\n",
    "The effective mission field will contain a higher level of discipline diversity than the constituent parts (although here we need to remember that the constituent parts might also be related to other missions we are not capturing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "_ = [ax.hist(df['entropy'],bins=20,color=col,alpha=0.6,density=True) for df,col in zip(mprof.sets,['blue','orange','green'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(100*mprof.discipline_mix.T).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mprof.impacts[1].plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission projects tend to generate more IP, less software and less products."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec project expansion\n",
    "\n",
    "We train a doc2vec model that looks for projects which are semantically closed to those we identify as highly relevant for the challenge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_documents = [TaggedDocument(doc,[num]) for num,doc in enumerate(corpus_tokenised.tokenised)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v = Doc2Vec(tagged_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ok, now I want to find the most similar documents a list of projects with \n",
    "#more than 1 occurrence of chronic related words and more than one occurrence of chronic occurring words\n",
    "projects['has_ai_n'],projects['has_chronic_n'] = [querier(corpus_tokenised.tokenised,keys,\n",
    "                                                     intersect=False) for keys in [ai_expanded,chronic_expanded]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_indices = list(projects.reset_index(drop=True).loc[(projects.has_ai_n>1) & (projects.has_chronic_n>1)].index)\n",
    "\n",
    "high_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This extracts those projects that are semantically closer to the initial list. \n",
    "\n",
    "close_docs = []\n",
    "\n",
    "for ind in high_indices:\n",
    "    sims = d2v.docvecs.most_similar([d2v.infer_vector(tagged_documents[ind].words)],topn=20)\n",
    "    \n",
    "    #Here we remove those projects that were already identified as high indices\n",
    "    close_docs.append([s[0] for s in sims if (s[0] not in high_indices) & (s[1]>0.7)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = set(flatten_list(close_docs))\n",
    "\n",
    "for t in projects.iloc[list(sims)]['title']:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of them look quite relevant. Others, not so much. I suppose the interdisciplinarity of the projects is pulling the vectors in multiple dimensions at the same time. Poor them!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Next steps (Friday)\n",
    "\n",
    "* Produce a better query system: a better seed list would help.\n",
    "* Package the descriptive stat generator into a function or class. This should be generic enough to accept dfs from other sources (eg H2020 and OpenAIRE).\n",
    "* Reimplement and test the above. Check how many of the new projects were already captured by the two broader categories\n",
    "* Organisation visualisation\n",
    "\n",
    "### Other combinations\n",
    "* Integrate with TRL analysis\n",
    "* Integrate with SDG analysis\n",
    "* Check social media discussion around papers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ageing and inclusion/inequality (crude keyword search-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_expanded = synonym_chaser(seed_list=['ageing','aging'],model=w2v,similarity=0.8)\n",
    "inclusion_expanded = synonym_chaser(seed_list=['inclusion','inclusiveness','inclusive','inequality'],model=w2v,similarity=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects['has_age'],projects['has_inclusion'] = [querier(corpus_tokenised.tokenised,keys) for keys in [age_expanded,inclusion_expanded]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(projects['has_age']>0,projects['has_inclusion']>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects.loc[(projects['has_age']>0) & (projects['has_inclusion']>0)].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1100*2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
